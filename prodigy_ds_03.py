# -*- coding: utf-8 -*-
"""PRODIGY_DS_03.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/137IiwcKnvMbUzqgkmvixUyk9aoyMYN0S

**Project Title:** **Predicting Client Subscription Using Machine Learning**


**Problem Statement**

A financial institution runs marketing campaigns to encourage clients to subscribe to long-term deposit accounts. However, reaching out to uninterested clients is costly and inefficient. The goal of this project was to build a machine learning model that accurately predicts whether a client will subscribe‚Äîhelping the company target the right people and optimize their resources.

**Tools and Technologies**

Python

Pandas, NumPy (data manipulation)

Matplotlib & Seaborn (visualization)

Scikit-learn (modeling)

Random Forest Classifier (best performing model)

GridSearchCV (hyperparameter tuning)



**Process**

Loading the data

Checking for duplicates and missing values

Exploratory data analysis

Converted categorical data using one-hot encoding

Scaled numerical features

Split data into training and test sets (80/20)

Model Building & Tuning

Used GridSearchCV for hyperparameter tuning

Evaluated using Accuracy, Precision, Recall, and F1-score

Results


‚úÖ Best Model: Random Forest Classifier

üéØ Accuracy: 82.7%

üîç Recall for Subscriptions: 85% (very few missed actual subscribers)

üéØ Precision for Subscriptions: 80% (few false alarms)


This means the model can effectively help the company focus its efforts on likely subscribers, saving time and marketing budget.

**Key Insights**


The most influential features included: duration, contact type, previous campaign outcome, and number of contacts.

Clients who were previously contacted and had longer call durations were more likely to subscribe.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('bank.csv')
df.info()

df.describe()

df.isnull().sum()

df.duplicated().sum()

plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='job', hue='deposit')
plt.title('Job Type vs Deposit Subscription')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 5))
sns.countplot(data=df, x='education', hue='deposit')
plt.title('Education Level vs Deposit Subscription')
plt.show()

plt.figure(figsize=(8, 4))
sns.countplot(data=df, x='marital', hue='deposit')
plt.title('Marital Status vs Deposit Subscription')
plt.show()

sns.countplot(data=df, x='housing', hue='deposit')
plt.title('Housing Loan vs Deposit Subscription')
plt.show()

sns.histplot(data=df, x='age', kde=True, bins=30)
plt.title('Age Distribution')
plt.show()

sns.boxplot(data=df, x='deposit', y='duration')
plt.title('Call Duration by Deposit Outcome')
plt.show()

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

sns.pairplot(df[['age', 'balance', 'duration', 'campaign', 'deposit']], hue='deposit')
plt.show()

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
for col in df.select_dtypes(include='object'):
    df[col] = label_encoder.fit_transform(df[col])

print(df.columns.tolist())

X = df.drop('deposit', axis=1)  # Features
y = df['deposit']               # Target

from sklearn.preprocessing import LabelEncoder

# Make a copy so you don‚Äôt affect original data
X_encoded = X.copy()

# Apply LabelEncoder to each object column
label_encoder = LabelEncoder()
for column in X_encoded.select_dtypes(include=['object']).columns:
    X_encoded[column] = label_encoder.fit_transform(X_encoded[column])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, classification_report

y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import pandas as pd
import matplotlib.pyplot as plt

importances = model.feature_importances_
features = X_encoded.columns

# Create a DataFrame for easy sorting
feat_imp = pd.DataFrame({'Feature': features, 'Importance': importances})
feat_imp = feat_imp.sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10,6))
plt.barh(feat_imp['Feature'], feat_imp['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.gca().invert_yaxis()
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier


params = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
}
grid = GridSearchCV(RandomForestClassifier(), params, cv=5, scoring='accuracy')
grid.fit(X_train, y_train)
print(grid.best_params_)

best_model = grid.best_estimator_
best_model.fit(X_train, y_train)

from sklearn.metrics import classification_report, accuracy_score

y_pred = best_model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

